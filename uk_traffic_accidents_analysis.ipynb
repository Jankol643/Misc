{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/daveianhickey/2000-16-traffic-flow-england-scotland-wales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os  # for checking file size\n",
    "\n",
    "import pandas as pd  # for dataframe\n",
    "import tkinter  # for selecting folder\n",
    "import tkinter.filedialog  # for selecting folder\n",
    "import tkinter.messagebox # for displaying error message\n",
    "import shutil  # for concatenating files\n",
    "import re  # for selecting correct files\n",
    "from itertools import groupby  # check if all headers are equal\n",
    "import folium # for drawing map\n",
    "from folium.plugins import FastMarkerCluster # for plotting many values\n",
    "import ntpath # for setting filename os-independently\n",
    "# for mapping accident locations to police forces\n",
    "import json\n",
    "from turfpy.measurement import boolean_point_in_polygon\n",
    "from geojson import Point, MultiPolygon, Feature\n",
    "import datetime # for converting to datetime\n",
    "import matplotlib # for viewing plotting settings\n",
    "\n",
    "output_foldername = 'output'\n",
    "output_filename = 'output.csv'\n",
    "error_msg = \"You have to specify a data path first!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_file_paths  # path to data files\n",
    "root = tkinter.Tk()\n",
    "data_path = tkinter.filedialog.askdirectory(mustexist=True)\n",
    "root.destroy()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_path)\n",
    "data_file_paths = list()\n",
    "regex = '[1-9]{1}[0-9]{2}[1-9]{1}'\n",
    "with os.scandir(data_path) as it:\n",
    "    for entry in it:\n",
    "        if entry.is_file() and entry.name.endswith(\".csv\") and re.search(regex, entry.name):\n",
    "            data_file_paths.append(entry.path)\n",
    "if not len(data_file_paths) > 0:\n",
    "    raise RuntimeError(\"No files found in folder \" + data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = os.path.dirname(data_file_paths[0])\n",
    "foldername = os.path.dirname(tmp)\n",
    "output_path = os.path.join(foldername, output_foldername)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "global concatenated_path\n",
    "concatenated_path = os.path.join(output_path, output_filename)\n",
    "global accident_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_headers_equal():\n",
    "    # checking if headers are equal\n",
    "    first_lines = list()\n",
    "    for file in data_file_paths:\n",
    "        with open(file, 'r') as f:\n",
    "            first_line = f.readline()\n",
    "            first_lines.append(first_line)\n",
    "\n",
    "    def all_equal(iterable):\n",
    "        g = groupby(iterable)\n",
    "        return next(g, True) and not next(g, False)\n",
    "\n",
    "    if not all_equal(first_lines):\n",
    "        raise RuntimeError(\"Headers are not equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_headers_equal()\n",
    "print(\"Converting files to dataframe...\")\n",
    "accident_data = pd.concat([pd.read_csv(\n",
    "    file, index_col='Accident_Index', parse_dates=True) for file in data_file_paths]).reset_index()\n",
    "print(\"Finished converting files to csv\")\n",
    "print('--------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(data_file_paths)\n",
    "total_filesize = 0\n",
    "for file in data_file_paths:\n",
    "    total_filesize += os.stat(file).st_size\n",
    "concatenated_filesize = os.stat(concatenated_path).st_size\n",
    "if os.path.isfile(concatenated_path) and concatenated_filesize != total_filesize:\n",
    "    with open(concatenated_path, 'wb') as wfd:\n",
    "        for f in data_file_paths:\n",
    "            index = data_file_paths.index(f) + 1\n",
    "            print(\"Concatenating file\", index,\n",
    "                    \"of\", length, \"files...\")\n",
    "            with open(f, 'rb') as fd:\n",
    "                shutil.copyfileobj(fd, wfd)\n",
    "    print(\"Finished concatenating files\")\n",
    "else:\n",
    "    print(\"No concatenation necessary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count missing values\n",
    "missing_table = accident_data.isnull().sum()\n",
    "sorted_missing = missing_table.sort_values(ascending=False)\n",
    "print(sorted_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_loc_data = accident_data.copy()\n",
    "accident_loc_data = accident_loc_data[['Latitude', 'Longitude']]\n",
    "accident_loc_data = accident_loc_data.dropna(axis=0, subset=['Latitude','Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = accident_loc_data[['Latitude', 'Longitude']]\n",
    "locations['Latitude'] = locations['Latitude'].astype(float)\n",
    "locations['Longitude'] = locations['Longitude'].astype(float)\n",
    "locationlist = locations.values.tolist()\n",
    "print(len(locationlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_lat, london_lng = 51.5074, -0.1278\n",
    "min_lat, min_lon = london_lat - 2, london_lng - 9\n",
    "max_lat, max_lon = london_lat + 10, london_lng + 1\n",
    "my_map = folium.Map(\n",
    "    zoom_start=5,\n",
    "    min_zoom=5,\n",
    "    max_bounds=True,\n",
    "    min_lat=min_lat,\n",
    "    max_lat=max_lat,\n",
    "    min_lon=min_lon,\n",
    "    max_lon=max_lon\n",
    "    )\n",
    "\n",
    "map_output_path = output_path + os.path.sep + 'uk_map.html'\n",
    "my_map.save(map_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add police district boundaries\n",
    "district_boundaries_filelist = list()\n",
    "police_districts_folder = foldername + os.path.sep + 'police districts'\n",
    "with os.scandir(police_districts_folder) as it:\n",
    "    for entry in it:\n",
    "        if entry.is_file() and os.stat(entry.path).st_size > 0:\n",
    "            if entry.name.endswith(\".geojson\"):\n",
    "                district_boundaries_filelist.append(entry.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all the points from the file to the map object using FastMarkerCluster\n",
    "my_map.add_child(FastMarkerCluster(locationlist))\n",
    "\n",
    "# save the map\n",
    "map_output_path = output_path + os.path.sep + 'accident_map_boundaries.html'\n",
    "my_map.save(map_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accidents by Speed Limit\n",
    "accident_data.Speed_limit.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map coordinates to police forces\n",
    "file_list = list()\n",
    "with_police_forces = accident_data.copy()\n",
    "with_police_forces['police_frce'] = 'pf1'\n",
    "\n",
    "def search_accident(data):\n",
    "    for feature in data['features']:\n",
    "        polygon = Feature(geometry=MultiPolygon([feature['geometry']['coordinates']], precision=15))\n",
    "        for idx, accident_line in enumerate(with_police_forces):\n",
    "            latitude = with_police_forces.loc[idx, 'Latitude']\n",
    "            longitude = with_police_forces.loc[idx, 'Longitude']\n",
    "            point = Feature(geometry=Point([latitude, longitude]))\n",
    "            if boolean_point_in_polygon(point, polygon):\n",
    "                print(feature['properties']['PFA20NM'])\n",
    "                #with_police_forces.loc[idx, 'police_frce'] = feature['properties']['PFA20NM']\n",
    "\n",
    "\n",
    "with open(district_boundaries_filelist[0], 'r') as f:\n",
    "    data = json.load(f)\n",
    "    search_accident(data)\n",
    "\n",
    "with_police_forces.to_csv(\"withAddedColumn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_date(row):\n",
    "    try:\n",
    "        newdate = datetime.datetime.strptime(row['Date'] + ' ' + str(row['Time']), '%d/%m/%Y %H:%M') \n",
    "    except:\n",
    "        row['Time'] = '00:00'\n",
    "        newdate = datetime.datetime.strptime(row['Date'] + ' ' + str(row['Time']), '%d/%m/%Y %H:%M')\n",
    "    return newdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data['date_time'] = accident_data.apply(lambda row: full_date(row),axis=1)\n",
    "accident_data['date_time'] = pd.to_datetime(accident_data['date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of accidents Time of the Day\n",
    "accident_data['date_time'].groupby(accident_data['date_time'].dt.hour).count().plot(\n",
    "    kind=\"bar\", xlabel=\"Time of day\", ylabel=\"Number of accidents\", title=\"Accidents per hour\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of accidents day of week\n",
    "accident_data['Day_of_Week'].groupby(accident_data['Day_of_Week']).count().plot(\n",
    "    kind=\"bar\", xlabel=\"Day of Week\", ylabel=\"Number of accidents\", title=\"Accidents per day of week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of accidents per month\n",
    "accident_data['date_time'].groupby(accident_data['date_time'].dt.month).count().plot(\n",
    "    kind=\"bar\", xlabel=\"Month\", ylabel=\"Number of accidents\", title=\"Accidents per month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of accidents per weather condition\n",
    "accident_data['Weather_Conditions'].groupby(accident_data['Weather_Conditions']).count().plot(\n",
    "    kind=\"bar\", xlabel=\"Weather condition\", ylabel=\"Number of accidents\", title=\"Accidents per weather condition\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
